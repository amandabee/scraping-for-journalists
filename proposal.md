Working Title: Scraping for Journalists

# Overview

"Web scraping," or writing small programs that automate the process of extracting information from websites,  is sometimes the specialized tool that makes an otherwise impossible story possible. In this five week class we'll use Python and Scraperwiki to write web scrapers of our own and write a story based on the data. Students do not need to be skilled developers already but they should have some experience peering under the hood of websites.


# Course Description

We will use the Python [BeautifulSoup library](http://www.crummy.com/software/BeautifulSoup/) and [Scraperwiki](https://scraperwiki.com/) to develop progressively more complex web scraping scripts.

You will not leave this course a Python programmer but you’ll have a solid handle on the Python you need to scrape the web and a great library of sample scrapers that you can adapt to your needs.

# Why we need this course

From big national wire services to small local newsrooms, reporters are using web scraping to get the story.  Daily Beast used web scraping to track presidential candidates' ground games in 2012[1]. The Baltimore Sun scraped Baltimore's finance department website after city officials said it wasn't possible to provide reporters with access to property tax records. Their series on the unintended consequences of a local tax credit[2] would not have been possible without that data.  Reporters at Reuters scraped the archives of multiple Yahoo Groups to gather data and stories for a five part series on disrupted adoptions[3]. These are just a few examples of newsrooms that have turned to web scraping when other reporting methods failed.

[1]  http://www.thedailybeast.com/articles/2012/10/19/ground-game-obama-opens-up-big-lead-in-state-headquarters.html

[2] http://www.baltimoresun.com/business/bs-bz-baltimore-homestead-credits-20111217,0,5608651.story

[3] http://www.reuters.com/investigates/adoption/#article/part1

## Suggested further reading:

+ <http://www.propublica.org/nerds/item/doc-dollars-guides-collecting-the-data>

+ <http://www.propublica.org/nerds/item/the-coders-cause-in-dollars-for-docs>

+ <http://source.mozillaopennews.org/en-US/learning/freeing-plum-book/>

+ <http://j-source.ca/article/web-scraping-how-journalists-get-their-own-data>

# Faculty

**Amanda Hickman** has spent more than a decade working at the intersection of journalism and civic engagement, reporting on local and international events. She co-developed the data module that CUNY Journalism School uses in every FMS class, and teaches data visualization and newsgames at the school.

Amanda helped launch [The New York World](http://www.thenyworld.com), and was program director at DocumentCloud, a Knight News Challenge funded project that reporters around the world are using to analyze, annotate, and publish primary source documents.

**Sandeep Junnarkar** Bio TK

# Learning Outcomes
You will not leave this course a Python programmer but you’ll have a solid handle on the Python you need to scrape the web and a great library of sample scrapers that you can adapt to your needs.

+ Basic facility with Python as a tool for extracting structured data from web pages into a CSV.  
+ Intro to the NICAR community;  
+ Learning to ask for help when something seems possible but you don't know how to do it.  
+ Understand options for scheduling repeat-scrapings 

We’ll explore tools that are available for scraping PDFs, but we don’t have any silver bullets for extracting structured data from PDFs.

# Prerequisites
Students do not need to be skilled developers already but they should have some experience peering under the hood of websites.

Students with some coding experience will be able to progress faster and tackle more complex

# Potential Assignments
Students will tackle progressively more complex scraping challenges as the course progresses.

# Possible in-class exercises
Each week, we’ll spend some hands-on time advancing our scraping skills, and reinforce those skills with weekly hands-on homework assignments.

# Week-by-Week
## Week 1: The Basics
Use scraperwiki to try out a simple table scraper. We’ll scrape data we *could* just cut and paste, just to get our feet wet.

We'll also play some with regular expressions, which we'll need eventually. 

## Week 2: Iterating through Pages and Asking for Help

Often, data that is published online could just be cut and pasted from a web based table, but it goes on for hundreds of pages. We’ll look at tools you can use to iterate through and scrape many, many pages at once.

We’ll also talk about building in sanity checks so that you know whether or not you’ve captured the full set of records.

We’ll look at resources for getting assistance online and asking good questions.

## Week 3: PDFs

We’ll look at [Tabula](http://tabula.nerdpower.org) as a tool for scraping simple tables from PDFs and explore Scraperwiki’s PDF tools (which should be out by then!)

We don’t have any silver bullets for PDF data extraction.

## Week 4: Scheduling Scraping

Sometimes the real value of scraping is in writing a widget that will go back to the same site every day (or week or month or hour) and grab a single value, adding it to a growing data store.

## Week 5: Special Problems

Students will be encouraged to identify scrape-able data sets, we’ll tackle some of the hurdles that surface in those.



